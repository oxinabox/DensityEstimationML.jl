{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "This is inspired by \n",
    "Article (likas2001probability) Likas, A. Probability density estimation using artificial neural networks Computer physics communications, Elsevier, 2001, 135, 167-175\n",
    "\n",
    "But rather than estimating the working with a network, we will instead work with its derivitive.\n",
    "This will let us replace their integration with a derivative.\n",
    "\n",
    "Note that this method only works for compact supports\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "they use the PDF is given by $$p_h(x,p) = \\dfrac{h(x,p)}{\\int_S h(z,p) dz}$$\n",
    "and in their case $h=N(x,p)$  a neural network with weight and bias parameters $p$.\n",
    "Where $S$ is a compact support. (That means bounded)\n",
    "\n",
    "\n",
    "But if instead we say $h=\\frac{\\partial N(x,p)}{\\partial x}$,\n",
    "\n",
    "then $$p_h(x,p) = \\dfrac{h(x,p)}{\\int_S h(z,p)}=\\dfrac{\\frac{\\partial N(x,p)}{\\partial x}}{N(max(S),p) - N(min(S), p)}$$\n",
    "\n",
    "The denominator is ofcourse more complex for non-1D values of S.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function given is the negative log-likelihood of the set of training samples $X$\n",
    "$$L(p) = -\\sum_{\\forall x \\in X} ln(h(x,p))  + |X| ln(\\int_S h(z,p) dx)$$\n",
    "\n",
    "Which befomes:\n",
    "\n",
    "$$L(p) = -\\sum_{\\forall x \\in X} log(\\frac{\\partial N(x,p)}{\\partial x})  + |X|(ln(N(max(S),p)-N(min(S),p)) dx$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using StatsBase\n",
    "using Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using TensorFlow\n",
    "using MLDataUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DensityEstimationML\n",
    "using Plots\n",
    "\n",
    "using MacroTools\n",
    "macro plot(ex)\n",
    "    @capture(ex, (x_, [ys__], tail__ )) \n",
    "    labels = repr.(ys)\n",
    "    ys_expr = Expr(:hvect, ys...)\n",
    "    labels_expr = Expr(:vect, repr.(ys)...)\n",
    "    Expr(:call, :plot, x, ys_expr, tail..., Expr(:kw, :labels, labels_expr))\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "immutable NeuralDensityEstimator\n",
    "    sess::Session\n",
    "    \n",
    "    #Network nodes\n",
    "    optimizer::Tensor\n",
    "    t::Tensor\n",
    "    pdf::Tensor\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function Distributions.pdf(est::NeuralDensityEstimator, t::Real)\n",
    "    gr = est.sess.graph\n",
    "    ts = reshape([t], (1,1))\n",
    "    run(est.sess, est.pdf, Dict(est.t=>ts))[1]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function StatsBase.fit!(estimator::NeuralDensityEstimator, observations;\n",
    "    epochs = 20,\n",
    "    batch_size = 1024)\n",
    "    \n",
    "    gr = estimator.sess.graph\n",
    "    for ii in 1:epochs\n",
    "        loss_o = run(estimator.sess, \n",
    "            [gr[\"true_loss\"], gr[\"working_loss\"],gr[\"denominator\"], estimator.optimizer],\n",
    "            Dict(estimator.t=>observations'))\n",
    "        println(\"Epoch $ii: loss: $(loss_o)\")\n",
    "    end\n",
    "    estimator\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function NeuralDensityEstimator(prob_layer_sizes, support_min, support_max)\n",
    "    sess = Session(Graph())\n",
    "    @tf begin\n",
    "        t = placeholder(Float32, shape=[1, -1])\n",
    "        zsmin = [support_min*ones(Tensor,(1,1))]\n",
    "        zsmax = [support_max*ones(Tensor,(1,1))]\n",
    "        zt = [t]\n",
    "        \n",
    "        layer_sizes= [1; prob_layer_sizes; 1]\n",
    "        act_funs = Vector{Function}(length(layer_sizes)-1)\n",
    "        act_funs[:] = nn.sigmoid\n",
    "        act_funs[end] = exp\n",
    "        for ii in 2:length(layer_sizes)\n",
    "            below_size = layer_sizes[ii-1]\n",
    "            above_size = layer_sizes[ii]\n",
    "            \n",
    "            act_fun = act_funs[ii-1]\n",
    "            \n",
    "            Wii = get_variable(\"W_$ii\", [above_size, below_size], Float32)\n",
    "            bii = if ii!=length(layer_sizes)\n",
    "                get_variable(\"b_$ii\", [above_size, 1], Float32)\n",
    "            else\n",
    "                zeros(Tensor, 1,1) #Final layer has no bias\n",
    "            end\n",
    "            push!(zt, act_fun(Wii*zt[end] .+ bii))\n",
    "            push!(zsmin, act_fun(Wii*zsmin[end] .+ bii))\n",
    "            push!(zsmax, act_fun(Wii*zsmax[end] .+ bii))\n",
    "        end\n",
    "        \n",
    "        ysmin = zsmin[end]\n",
    "        ysmax = zsmax[end]\n",
    "        yt = zt[end]\n",
    "        \n",
    "        \n",
    "        denominator = (ysmax-ysmin) #area\n",
    "        numerator = identity(gradients(yt,t))\n",
    "        pdf =numerator/denominator\n",
    "        \n",
    "        n_points = TensorFlow.shape(t)[2]\n",
    "        true_loss=-reduce_sum(log(numerator)) + n_points.*log(denominator)\n",
    "        \n",
    "        area_loss = (1f0.-denominator)^2\n",
    "        working_loss = true_loss + 0.1*area_loss\n",
    "        optimizer = train.minimize(train.AdamOptimizer(), working_loss)\n",
    "    end\n",
    "    \n",
    "    run(sess, global_variables_initializer())\n",
    "    \n",
    "    NeuralDensityEstimator(sess, optimizer, t, pdf)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = GenerateDatasets.magdon_ismail_and_atiya\n",
    "data = dataset()\n",
    "est = NeuralDensityEstimator([1024], support(dataset)...)\n",
    "fit!(est, data; epochs=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GenerateDatasets.magdon_ismail_and_atiya\n",
    "data = dataset()\n",
    "est = NeuralDensityEstimator([1024], -16,16)\n",
    "fit!(est, data; epochs=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x->pdf(est,x), xlims= support(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?TensorFlow.Ops.select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x->x<0? -x:log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
